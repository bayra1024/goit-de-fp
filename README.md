# goit-de-fp


docker-compose create - загрузка образів докера 
docker-compose start - запуск образів

### Висновки по частині 1:

1. **Зчитування фізичних показників атлетів з MySQL:**

   - Дані з таблиці `olympic_dataset.athlete_bio` успішно завантажені

2. **Фільтрація даних атлетів:**

   - Дані, з порожніми значення зросту (`height`) та ваги (`weight`) видалено.

3. **Робота з даними результатів змагань:**

   - Дані з таблиці `athlete_event_results` були завантажені з MySQL з використанням партицій.
   - Дані записані у Kafka-топік `athlete_event_results` у форматі JSON.
   - З Kafka-топіку `athlete_event_results` дані було зчитано у Spark-стрім у форматі JSON.

4. **Об’єднання даних:**

   - Дані результатів змагань об’єднані з даними з MySQL (`athlete_bio`) за ключем `athlete_id`.

5. **Розрахунок середніх показників:**

   - Розраховано середні значення зросту (`height`) і ваги (`weight`), типу медалі (або її відсутності), статі (`sex`) та країни (`country_noc`).

6. **Стрім даних:**
   - Результати обробки було організовано у стрімінговому форматі:
     - Дані були записані у вихідний Kafka-топік `lina_aggregated_athlete_stats`
     - Дані були записані у базу даних MySQL (`lina_aggregated_athlete_stats`)

p_1.1-p_1.6 - скріншоти обробки засобами спарк та кафка

### Висновки по частині 2:

У цьому проєкті створено простий multi-hop datalake:

1. **Landing to Bronze**: дані завантажуються з FTP-сервера в форматі Parquet.

2. **Bronze to Silver**: очистка текстових колонок, видалення дублікатів і перетворення даних у більш стандартизований формат.

3. **Silver to Gold**: аналітично-готовий набір, що містить середні показники ваги та зросту для різних категорій спортсменів.

4. **Оркестрація через Airflow DAG**: DAG автоматизує всі етапи ETL-процесу

p_2.1-p_2.4 - скріншоти обробки даних засобами airflow по рівням лендінгу, bronze, silver, gold.